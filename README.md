# Poetry Generation using Language Modeling (n-grams)

Language modeling is a statistical measure of finding and assigning probabilities to words and sentences. Given a history of previous word(s), it provides an estimate on what word might come next. A traditional method of implementing a language model is via n-grams, pertaining unigrams, bigrams, trigrams and so forth.
### Implementation:
In this task, the first three n-grams are implemented that follow the following steps:
•	Tokenizing the training data using spacy
Using spaCy, the tokens were generated and the corpus was cleaned by removing punctuations like '.', ',', ':', '?', '!', ';', '-', '\"'

•	Generating the n-gram models
In case of unigrams, the tokens itself are the unigrams as this model works at the level of individual words. Bigrams were generated using the summation of two words, whereas trigrams were generated by iterating the combination of three words in order.

•	Finding probabilities of each model on the training data
To find probabilities, firstly a dictionary was maintained to calculate the frequency of unigram, bigram and trigram using Counter from collections. Then for unigram, the probability of each word is the frequency itself. To normalize it, these frequencies are divided by the number of vocabulary/unique words. Hence:
Probability of unigrams = Frequency of each unigram/length of tokens
Similarly, for bigram, 
Probability of bigrams = Frequency of each bigram/Frequency of each unigram
Likewise, for trigram,
         Probability of trigrams = Frequency of each trigram/Frequency of each bigram

•	Generating poetry using each model:
Unigram:
To generate poetry using unigrams, we start each sentence with a random unigram word. The next words are also generated randomly from unigrams list, since unigrams do not utilize previous history of words. 
Bigram:
To generate poetry using bigrams, we start each sentence with a random unigram word. The next words are generating by first checking if the previous word is present in the bigram probability. If it exists, store all key-value pairs. For example:
If the start word is warriors, it checks if this word exists as a first word in the key of bigram probabilities, i.e., ‘warriors of’, ‘warriors life’. 
Then, select the word with maximum probability from this key-value pair and append this as a next word of the sentence. Iterate it to get the newly generated poem using bigrams.
Trigram:
Similar to above, we start each sentence with a random bigram. Based on the previous two words, we generate the next word by choosing the word with maximum probability in trigrams dictionary until the poem is formed.

•	Calculating perplexity of the newly generated text upon smoothing
For each n-gram model,
Split the poem into sentences. For each sentence, if the word is not in model’s probability dictionary (i.e., unknown word), then set boolean = True, meaning we need to apply smoothing. If true, apply probabilities with smoothed values, else simply apply values from model’s probabilistic values. 
Method to apply smoothing: 
Laplace Add-1 smoothing is implemented which is applied as follows:
          Smoothed Probability of unigrams = Frequency of each unigram + 1/length of tokens + length of unigram probabilities
 
Smoothed Probability of bigrams = Frequency of each bigram + 1/frequency of each unigram + length of bigram probabilities

Smoothed Probability of trigrams = Frequency of each trigram + 1/frequency of each bigram + length of trigram probabilities

•	Tokenizing unseen test data using spacy
Using spaCy, the tokens were generated and the corpus was cleaned by removing punctuations like '.', ',', ':', '?', '!', ';', '-', '\"'

•	Calculating perplexities using trained probabilities upon smoothing
Perplexities are calculated the same way as trained data. It uses the trained unigrams, bigrams and trigrams probabilities and the provided test data.

### Results:
The performance of the n-gram models is measured using perplexity. The lesser the perplexity, the better is the model.

Newly Generated Poems on Trained data:
Among the newly generated poems using n-grams, the bigram model performed the best as it has the least perplexity (around 1). 
Test data:
In the test data, the unigram model performs the best whereas the bigram and trigram model do not perform good. This is because the test data is entirely different from the training data i.e., more unknowns and hence perplexity is not a good measure to calculate. 

### Challenges:
Following are the challenges encountered during this:
1. Choosing the smoothing method to see which smoothing method performs better. Laplace (Add-one) smoothing tends to reassign too much mass to unseen events, so can be adjusted by normalizing in between range 0-1 multiplied by V. 
2. Instead of choosing Laplace Add-one, using Add-k value worked. Different values of k were applied to get the least perplexity of the unseen sentences. K = 0.8 was served best in generating newly generated data. 
3. While generating new poems, probabilities with max values are chosen to generate different phrases, that lessens the values. 
